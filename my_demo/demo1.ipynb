{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "12.4\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # 查看 PyTorch 版本\n",
    "print(torch.version.cuda)  # 查看 CUDA 版本\n",
    "print(torch.cuda.is_available())  # 检查 CUDA 是否可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/video-r1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-15 15:32:19,549] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/video-r1/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/envs/video-r1/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path= \"/data/guojian.li/Weight/Qwen2.5-VL-7B-COT-SFT/\"\n",
    "model_path = \"/data/guojian.li/Weight/Video-R1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path, torch_dtype=\"auto\", device_map=\"cuda:7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter visual.patch_embed.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.0.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.0.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.0.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.0.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.0.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.0.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.0.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.0.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.0.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.0.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.0.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.0.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.1.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.1.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.1.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.1.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.1.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.1.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.1.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.1.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.1.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.1.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.1.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.1.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.2.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.2.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.2.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.2.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.2.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.2.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.2.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.2.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.2.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.2.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.2.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.2.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.3.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.3.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.3.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.3.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.3.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.3.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.3.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.3.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.3.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.3.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.3.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.3.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.4.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.4.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.4.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.4.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.4.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.4.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.4.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.4.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.4.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.4.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.4.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.4.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.5.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.5.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.5.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.5.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.5.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.5.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.5.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.5.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.5.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.5.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.5.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.5.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.6.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.6.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.6.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.6.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.6.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.6.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.6.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.6.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.6.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.6.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.6.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.6.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.7.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.7.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.7.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.7.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.7.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.7.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.7.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.7.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.7.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.7.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.7.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.7.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.8.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.8.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.8.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.8.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.8.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.8.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.8.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.8.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.8.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.8.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.8.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.8.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.9.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.9.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.9.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.9.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.9.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.9.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.9.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.9.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.9.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.9.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.9.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.9.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.10.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.10.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.10.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.10.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.10.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.10.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.10.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.10.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.10.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.10.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.10.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.10.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.11.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.11.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.11.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.11.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.11.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.11.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.11.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.11.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.11.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.11.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.11.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.11.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.12.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.12.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.12.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.12.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.12.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.12.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.12.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.12.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.12.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.12.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.12.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.12.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.13.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.13.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.13.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.13.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.13.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.13.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.13.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.13.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.13.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.13.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.13.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.13.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.14.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.14.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.14.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.14.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.14.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.14.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.14.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.14.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.14.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.14.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.14.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.14.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.15.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.15.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.15.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.15.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.15.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.15.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.15.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.15.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.15.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.15.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.15.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.15.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.16.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.16.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.16.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.16.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.16.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.16.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.16.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.16.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.16.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.16.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.16.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.16.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.17.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.17.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.17.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.17.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.17.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.17.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.17.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.17.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.17.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.17.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.17.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.17.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.18.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.18.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.18.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.18.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.18.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.18.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.18.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.18.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.18.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.18.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.18.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.18.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.19.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.19.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.19.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.19.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.19.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.19.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.19.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.19.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.19.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.19.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.19.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.19.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.20.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.20.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.20.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.20.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.20.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.20.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.20.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.20.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.20.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.20.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.20.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.20.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.21.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.21.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.21.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.21.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.21.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.21.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.21.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.21.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.21.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.21.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.21.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.21.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.22.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.22.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.22.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.22.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.22.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.22.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.22.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.22.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.22.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.22.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.22.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.22.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.23.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.23.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.23.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.23.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.23.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.23.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.23.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.23.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.23.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.23.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.23.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.23.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.24.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.24.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.24.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.24.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.24.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.24.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.24.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.24.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.24.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.24.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.24.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.24.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.25.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.25.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.25.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.25.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.25.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.25.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.25.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.25.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.25.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.25.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.25.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.25.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.26.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.26.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.26.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.26.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.26.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.26.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.26.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.26.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.26.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.26.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.26.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.26.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.27.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.27.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.27.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.27.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.27.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.27.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.27.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.27.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.27.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.27.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.27.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.27.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.28.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.28.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.28.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.28.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.28.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.28.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.28.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.28.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.28.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.28.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.28.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.28.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.29.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.29.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.29.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.29.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.29.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.29.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.29.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.29.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.29.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.29.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.29.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.29.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.30.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.30.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.30.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.30.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.30.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.30.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.30.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.30.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.30.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.30.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.30.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.30.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.31.norm1.weight dtype: torch.float32\n",
      "Parameter visual.blocks.31.norm2.weight dtype: torch.float32\n",
      "Parameter visual.blocks.31.attn.qkv.weight dtype: torch.float32\n",
      "Parameter visual.blocks.31.attn.qkv.bias dtype: torch.float32\n",
      "Parameter visual.blocks.31.attn.proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.31.attn.proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.31.mlp.gate_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.31.mlp.gate_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.31.mlp.up_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.31.mlp.up_proj.bias dtype: torch.float32\n",
      "Parameter visual.blocks.31.mlp.down_proj.weight dtype: torch.float32\n",
      "Parameter visual.blocks.31.mlp.down_proj.bias dtype: torch.float32\n",
      "Parameter visual.merger.ln_q.weight dtype: torch.float32\n",
      "Parameter visual.merger.mlp.0.weight dtype: torch.float32\n",
      "Parameter visual.merger.mlp.0.bias dtype: torch.float32\n",
      "Parameter visual.merger.mlp.2.weight dtype: torch.float32\n",
      "Parameter visual.merger.mlp.2.bias dtype: torch.float32\n",
      "Parameter model.embed_tokens.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.0.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.0.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.0.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.0.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.0.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.0.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.0.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.0.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.0.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.0.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.0.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.0.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.1.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.1.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.1.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.1.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.1.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.1.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.1.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.1.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.1.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.1.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.1.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.1.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.2.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.2.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.2.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.2.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.2.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.2.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.2.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.2.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.2.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.2.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.2.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.2.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.3.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.3.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.3.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.3.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.3.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.3.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.3.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.3.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.3.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.3.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.3.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.3.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.4.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.4.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.4.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.4.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.4.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.4.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.4.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.4.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.4.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.4.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.4.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.4.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.5.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.5.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.5.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.5.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.5.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.5.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.5.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.5.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.5.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.5.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.5.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.5.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.6.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.6.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.6.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.6.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.6.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.6.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.6.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.6.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.6.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.6.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.6.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.6.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.7.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.7.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.7.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.7.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.7.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.7.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.7.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.7.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.7.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.7.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.7.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.7.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.8.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.8.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.8.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.8.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.8.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.8.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.8.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.8.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.8.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.8.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.8.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.8.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.9.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.9.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.9.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.9.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.9.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.9.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.9.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.9.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.9.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.9.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.9.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.9.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.10.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.10.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.10.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.10.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.10.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.10.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.10.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.10.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.10.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.10.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.10.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.10.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.11.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.11.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.11.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.11.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.11.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.11.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.11.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.11.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.11.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.11.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.11.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.11.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.12.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.12.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.12.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.12.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.12.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.12.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.12.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.12.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.12.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.12.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.12.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.12.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.13.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.13.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.13.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.13.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.13.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.13.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.13.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.13.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.13.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.13.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.13.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.13.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.14.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.14.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.14.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.14.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.14.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.14.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.14.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.14.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.14.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.14.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.14.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.14.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.15.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.15.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.15.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.15.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.15.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.15.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.15.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.15.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.15.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.15.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.15.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.15.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.16.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.16.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.16.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.16.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.16.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.16.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.16.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.16.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.16.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.16.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.16.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.16.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.17.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.17.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.17.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.17.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.17.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.17.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.17.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.17.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.17.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.17.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.17.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.17.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.18.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.18.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.18.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.18.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.18.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.18.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.18.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.18.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.18.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.18.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.18.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.18.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.19.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.19.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.19.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.19.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.19.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.19.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.19.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.19.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.19.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.19.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.19.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.19.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.20.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.20.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.20.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.20.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.20.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.20.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.20.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.20.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.20.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.20.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.20.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.20.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.21.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.21.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.21.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.21.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.21.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.21.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.21.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.21.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.21.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.21.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.21.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.21.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.22.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.22.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.22.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.22.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.22.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.22.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.22.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.22.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.22.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.22.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.22.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.22.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.23.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.23.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.23.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.23.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.23.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.23.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.23.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.23.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.23.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.23.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.23.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.23.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.24.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.24.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.24.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.24.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.24.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.24.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.24.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.24.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.24.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.24.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.24.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.24.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.25.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.25.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.25.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.25.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.25.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.25.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.25.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.25.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.25.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.25.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.25.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.25.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.26.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.26.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.26.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.26.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.26.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.26.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.26.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.26.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.26.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.26.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.26.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.26.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.27.self_attn.q_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.27.self_attn.q_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.27.self_attn.k_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.27.self_attn.k_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.27.self_attn.v_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.27.self_attn.v_proj.bias dtype: torch.bfloat16\n",
      "Parameter model.layers.27.self_attn.o_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.27.mlp.gate_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.27.mlp.up_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.27.mlp.down_proj.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.27.input_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.layers.27.post_attention_layernorm.weight dtype: torch.bfloat16\n",
      "Parameter model.norm.weight dtype: torch.bfloat16\n",
      "Parameter lm_head.weight dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# 检查模型的参数数据类型\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter {name} dtype: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"/data/guojian.li/Project/Video-R1/my_demo/data/image.png\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Are there the same number of cylinders and gray matte balls?\"},\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"To determine if there are the same number of cylinders and gray matte balls in the image, let's count each type separately.\\n\\nFirst, let's count the cylinders:\\n- There is one small cyan cylinder.\\n- There is one small green cylinder.\\n\\nSo, there are 2 cylinders in total.\\n\\nNext, let's count the gray matte balls:\\n- There is one small gray matte ball.\\n\\nSo, there is 1 gray matte ball in total.\\n\\nComparing the counts:\\n- Cylinders: 2\\n- Gray matte balls: 1\\n\\nSince 2 is not equal to 1, there are not the same number of\"]\n"
     ]
    }
   ],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n",
    "# cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"To determine if there are the same number of cylinders and gray matte balls in the image, let's count each type separately.\\n\\nFirst, let's count the cylinders:\\n- There is a small cyan cylinder on the left side of the image.\\n- There is a green cylinder near the center-right of the image.\\nSo, there are 2 cylinders in total.\\n\\nNext, let's count the gray matte balls:\\n- There is one small gray matte ball located towards the back of the image.\\nSo, there is 1 gray matte ball in total.\\n\\nNow, we compare the counts:\\n- The number of cylinders is 2.\\n- The number of gray matte balls is 1.\\n\\nSince 2 is not equal to 1, there are not the same number of cylinders and gray matte balls.\\n\\nTherefore, the answer is no.\"]\n"
     ]
    }
   ],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n",
    "# r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages containing a video url and a text query\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"/data/guojian.li/Project/Video-R1/my_demo/data/emotion2.mp4\",\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "         151645,    198, 151644,    872,    198, 151652, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656, 151656,\n",
       "         151656, 151656, 151656, 151653,  74785,    419,   2766,     13, 151645,\n",
       "            198, 151644,  77091,    198]], device='cuda:5'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]], device='cuda:5'), 'pixel_values_videos': tensor([[-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "        [-1.7923, -1.7923, -1.7923,  ..., -1.1532, -1.1389, -1.1247],\n",
       "        ...,\n",
       "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802],\n",
       "        [-1.7923, -1.7923, -1.7923,  ..., -1.4802, -1.4802, -1.4802]],\n",
       "       device='cuda:5'), 'video_grid_thw': tensor([[ 2, 16, 30]], device='cuda:5'), 'second_per_grid_ts': [1.4180833333333334]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    \n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    **video_kwargs,\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The video appears to be a scene from a movie or TV show, featuring two characters in what looks like an indoor setting with warm lighting and wooden beams on the ceiling. The character in the foreground is wearing a brown jacket over a pink shirt and seems to be speaking to someone off-camera. The subtitles at the bottom of the screen indicate that one character is asking another named James to lower his voice because he\\'s \"freaking out Jay.\" This suggests a tense or serious conversation taking place, possibly involving a third person named Jay who might be feeling distressed by the volume of the speaker. The overall atmosphere of the scene feels dramatic and focused']\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The video appears to be a scene from a movie or TV show, featuring two characters in what looks like an indoor setting with warm lighting and wooden beams on the ceiling. One character is wearing a brown jacket over a red shirt, while the other character's back is turned to the camera. The subtitles suggest that one character is asking the other to lower their voice because they are scaring someone named Jay. The atmosphere seems tense, possibly indicating a moment of conflict or urgency between the characters. The background includes some artwork on the wall, adding to the indoor setting's ambiance.\"]\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'video-r1 (Python 3.11.11)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n video-r1 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-r1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
